{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Big Data: Hadoop, Spark, NoSQL and IoT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable high-res images in notebook \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.1 Introduction\n",
    "### Big Data\n",
    "* Previous data-science case studies all focused on AI\n",
    "* Here, we focus on the **big-data infrastructure that supports AI solutions**\n",
    "* As **data grows exponentially**, we want to **learn** from that data&mdash;and at **blazing speed**\n",
    "* Done with **sophisticated algorithms**, hardware, software and networking designs\n",
    "* With **big data**, **machine learning** and **deep learning** can be even **more effective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 NoSQL Big-Data Databases (1 of 2)\n",
    "* **Relational databases** store data in rectangular **tables**\n",
    "* **Not efficient** as **data volume** and the numbers of **tables** and **relationships between them** increases\n",
    "* Most data produced today is \n",
    "    * **Unstructured**&mdash;**photos**, **videos** and **natural language** (social-media posts, texts, ...), or\n",
    "    * **Semi-structured**&mdash;**JSON** and **XML** documents\n",
    "* **Metadata** adds structure to **unstructured data**, making it **semi-structured**\n",
    "    * **Tweets** (as you saw earlier)\n",
    "    * **YouTube videos**&mdash;**who posted** and **when**, **title**, **description**, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 NoSQL Big-Data Databases (2 of 2)\n",
    "* **NoSQL databases** are designed for \n",
    "    * **unstructured** and **semi-structured big-data** \n",
    "    * big data **storage and processing demands**\n",
    "* **Big data** requires **massive databases**, which can be spread across data centers worldwide in huge **clusters** of commodity computers\n",
    "* The name **NoSQL** originally meant what its name implies\n",
    "* With **growing use of SQL in big data**—such as **SQL on Hadoop** and **Spark SQL**—now it's said to stand for **“Not Only SQL”** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Major Types of **NoSQL Databases**\n",
    "* **key–value**\n",
    "* **document**\n",
    "* **columnar**\n",
    "* **graph**\n",
    "* Our NoSQL case study uses **MongoDB document database** &mdash; the most popular NoSQL database\n",
    "* For **overviews** of the **NoSQL database types** see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) (big data lesson coming soon) or [**Python for Programmers, Section 16.3**](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch16.xhtml#ch16lev1sec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.4 Case Study: A MongoDB JSON Document Database \n",
    "* Store and search **JSON** for **streamed tweets** about **100 U.S. senators**\n",
    "* Summarize **top 10** by **tweet count**\n",
    "* Display **interactive map** containing **tweet count summaries**\n",
    "* I **pre-executed this example** because we stream 10,000 tweets, which can take substantial time \n",
    "* **Possible enhancement** &mdash; Use **sentiment analysis** to count **positive**, **negative** and **neutral tweets** mentioning each senator’s **handle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Cloud-Based MongoDB Atlas Cluster \n",
    "* Requires **no installation** \n",
    "* Store up to **512MB of data**\n",
    "* Can store more with\n",
    "    * [**Free MongoDB Community Server**](https://www.mongodb.com/download-center/community), or \n",
    "    * **Paid MongoDB Atlas account**\n",
    "* **Creating your MongoDB Atlas cluster**\n",
    "    * I discuss the details of **signing up** for a MongoDB account, **creating the MongoDB Atlas Cluster**, **configuring** it and getting your **connection string** in my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) (big data lesson coming soon) and in [**Python for Programmers, Section 16.4.1**](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries Required for Interacting with MongoDB\n",
    "```\n",
    "conda install -c conda-forge pymongo\n",
    "conda install -c conda-forge dnspython\n",
    "```\n",
    "* **`pymongo` library** &mdash; interact with **MongoDB databases** from Python\n",
    "* **`dnspython` library** &mdash; used as part of connecting to a **MongoDB Atlas Cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keys.py \n",
    "* **`keys.py`** must contain \n",
    "    * your **Twitter credentials** \n",
    "    * your **OpenMapQuest key** \n",
    "    * Your **MongoDB connection string** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4.2 Streaming Tweets into MongoDB\n",
    "### Use Tweepy to Authenticate with Twitter and Get the API Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(\n",
    "    keys.consumer_key, keys.consumer_secret)\n",
    "auth.set_access_token(keys.access_token, \n",
    "    keys.access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True, \n",
    "                 wait_on_rate_limit_notify=True)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Senators’ Data (1 of 2)\n",
    "* **`senators.csv`** (provided in notebook's folder) contains each senator's \n",
    "    * two-letter state code\n",
    "    * name\n",
    "    * party \n",
    "    * Twitter handle\n",
    "    * Twitter ID\n",
    "* **Twitter handle and ID** used to track tweets **to**, **from** and **mentioning** each senator \n",
    "* When following users via **numeric Twitter IDs**, must submit IDs as **strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Senators’ Data (2 of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "senators_df = pd.read_csv('senators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "senators_df['TwitterID'] = senators_df['TwitterID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Name</th>\n",
       "      <th>Party</th>\n",
       "      <th>TwitterHandle</th>\n",
       "      <th>TwitterID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>Richard Shelby</td>\n",
       "      <td>R</td>\n",
       "      <td>SenShelby</td>\n",
       "      <td>21111098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>Doug Jomes</td>\n",
       "      <td>D</td>\n",
       "      <td>SenDougJones</td>\n",
       "      <td>941080085121175552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>Lisa Murkowski</td>\n",
       "      <td>R</td>\n",
       "      <td>lisamurkowski</td>\n",
       "      <td>18061669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK</td>\n",
       "      <td>Dan Sullivan</td>\n",
       "      <td>R</td>\n",
       "      <td>SenDanSullivan</td>\n",
       "      <td>2891210047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AZ</td>\n",
       "      <td>Martha McSally</td>\n",
       "      <td>R</td>\n",
       "      <td>SenMcSallyAZ</td>\n",
       "      <td>2964949642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State            Name Party   TwitterHandle           TwitterID\n",
       "0    AL  Richard Shelby     R       SenShelby            21111098\n",
       "1    AL      Doug Jomes     D    SenDougJones  941080085121175552\n",
       "2    AK  Lisa Murkowski     R   lisamurkowski            18061669\n",
       "3    AK    Dan Sullivan     R  SenDanSullivan          2891210047\n",
       "4    AZ  Martha McSally     R    SenMcSallyAZ          2964949642"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senators_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the `pymongo` `MongoClient` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_client = MongoClient(keys.mongo_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get **`pymongo` `Database`** Object Representing the `senators` Database\n",
    "* **Creates the database** if it does not exist\n",
    "* Will be used to store the collection of **tweet JSON documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = atlas_client.senators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Tweet Stream\n",
    "* **`TweetListener`** uses the **`db` object** representing the **senators database** to store tweets \n",
    "    * Depending on the rate at which people are tweeting about the senators, it may take **minutes to hours** to get **10,000 tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweetlistener import TweetListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_limit = 10000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_stream = tweepy.Stream(api.auth, \n",
    "    TweetListener(api, db, tweet_limit)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Live Tweet Stream\n",
    "* Currently, can **track** up to **400 keywords** and **follow** up to **5,000 Twitter IDs** at a time\n",
    "    * **`track`** senators’ Twitter handles as keywords\n",
    "    * **`follow`** their **IDs**  \n",
    "* Together, this will get tweets **from**, **to** and **about** each senator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Screen name: Kathy Buckley\n",
      "     Created at: Mon Jul 29 15:28:42 +0000 2019\n",
      "Tweets received: 10000\n"
     ]
    }
   ],
   "source": [
    "twitter_stream.filter(track=senators_df.TwitterHandle.tolist(),\n",
    "    follow=senators_df.TwitterID.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ```\n",
    "    Screen name: XXXXXXX\n",
    "     Created at: Sun Dec 16 17:19:19 +0000 2018\n",
    "Tweets received: 1\n",
    "...\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class `TweetListener` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# tweetlistener.py\n",
    "\"\"\"TweetListener downloads tweets and stores them in MongoDB.\"\"\"\n",
    "import json\n",
    "import tweepy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class TweetListener(tweepy.StreamListener):\n",
    "    \"\"\"Handles incoming Tweet stream.\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def __init__(self, api, database, limit=10000):\n",
    "        \"\"\"Create instance variables for tracking number of tweets.\"\"\"\n",
    "        self.db = database\n",
    "        self.tweet_count = 0\n",
    "        self.TWEET_LIMIT = limit  # 10,000 by default\n",
    "        super().__init__(api)  # call superclass's init\n",
    "\n",
    "    def on_connect(self):\n",
    "        \"\"\"Called when your connection attempt is successful, enabling \n",
    "        you to perform appropriate application tasks at that point.\"\"\"\n",
    "        print('Successfully connected to Twitter\\n')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def on_data(self, data):\n",
    "        \"\"\"Called when Twitter pushes a new tweet to you.\"\"\"\n",
    "        self.tweet_count += 1  # track number of tweets processed\n",
    "        json_data = json.loads(data)  # convert string to JSON\n",
    "        self.db.tweets.insert_one(json_data)  # store in tweets collection\n",
    "        clear_output()  # ADDED: show one tweet at a time in Jupyter Notebook\n",
    "        print(f'    Screen name: {json_data[\"user\"][\"name\"]}') \n",
    "        print(f'     Created at: {json_data[\"created_at\"]}')         \n",
    "        print(f'Tweets received: {self.tweet_count}')         \n",
    "\n",
    "        # if TWEET_LIMIT is reached, return False to terminate streaming\n",
    "        return self.tweet_count < self.TWEET_LIMIT\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tweets for Each Senator (1 of 2)\n",
    "* MongoDB **text search** requires a **text index** specifying **document field(s) to search** \n",
    "\t* MongoDB [index types](https://docs.mongodb.com/manual/indexes), [text indexes](https://docs.mongodb.com/manual/core/index-text) and [operators](https://docs.mongodb.com/manual/reference/operator)\n",
    "* A **text index** is defined as a **tuple** containing **field name** to search and **index type** (`'text'`)\n",
    "* **Wildcard field name (\\$\\*\\*)** indexes **all** text fields for a **full-text search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$**_text'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tweets.create_index([('$**', 'text')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tweets for Each Senator (2 of 2)\n",
    "* Use **`tweets` `Collection`’s `count_documents` method** and **full-text search** to count the total number of documents in the collection that contain the specified text\n",
    "    * Find every **twitter handle** in `senators_df.TwitterHandle` column\n",
    "    * `{\"$text\": {\"$search\": senator}}` indicates that we’re **using the `text` index** to **`search`** for the value of **`senator`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for senator in senators_df.TwitterHandle:\n",
    "    tweet_counts.append(db.tweets.count_documents(\n",
    "        {\"$text\": {\"$search\": senator}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Tweet Counts for Each Senator \n",
    "* Create copy of **`DataFrame` `senators_df`** adding a new column of **`tweet_counts`** \n",
    "* Display the **top-10 senators by tweet count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts_df = senators_df.assign(Tweets=tweet_counts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Name</th>\n",
       "      <th>Party</th>\n",
       "      <th>TwitterHandle</th>\n",
       "      <th>TwitterID</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>TX</td>\n",
       "      <td>John Cornyn</td>\n",
       "      <td>R</td>\n",
       "      <td>JohnCornyn</td>\n",
       "      <td>13218102</td>\n",
       "      <td>1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>KY</td>\n",
       "      <td>Rand Paul</td>\n",
       "      <td>R</td>\n",
       "      <td>RandPaul</td>\n",
       "      <td>216881337</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CT</td>\n",
       "      <td>Christopher Murphy</td>\n",
       "      <td>D</td>\n",
       "      <td>ChrisMurphyCT</td>\n",
       "      <td>150078976</td>\n",
       "      <td>1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>KY</td>\n",
       "      <td>Mitch McConnell</td>\n",
       "      <td>R</td>\n",
       "      <td>SenateMajLdr</td>\n",
       "      <td>1249982359</td>\n",
       "      <td>1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NY</td>\n",
       "      <td>Chuck Schumer</td>\n",
       "      <td>D</td>\n",
       "      <td>SenSchumer</td>\n",
       "      <td>17494010</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FL</td>\n",
       "      <td>Marco Rubio</td>\n",
       "      <td>R</td>\n",
       "      <td>marcorubio</td>\n",
       "      <td>15745368</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FL</td>\n",
       "      <td>Rick Scott</td>\n",
       "      <td>R</td>\n",
       "      <td>SenRickScott</td>\n",
       "      <td>131546062</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>SC</td>\n",
       "      <td>Lindsey Graham</td>\n",
       "      <td>R</td>\n",
       "      <td>LindseyGrahamSC</td>\n",
       "      <td>432895323</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NJ</td>\n",
       "      <td>Cory Booker</td>\n",
       "      <td>D</td>\n",
       "      <td>CoryBooker</td>\n",
       "      <td>15808765</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CA</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>D</td>\n",
       "      <td>SenKamalaHarris</td>\n",
       "      <td>803694179079458816</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State                Name Party    TwitterHandle           TwitterID  \\\n",
       "84    TX         John Cornyn     R       JohnCornyn            13218102   \n",
       "33    KY           Rand Paul     R         RandPaul           216881337   \n",
       "12    CT  Christopher Murphy     D    ChrisMurphyCT           150078976   \n",
       "32    KY     Mitch McConnell     R     SenateMajLdr          1249982359   \n",
       "62    NY       Chuck Schumer     D       SenSchumer            17494010   \n",
       "17    FL         Marco Rubio     R       marcorubio            15745368   \n",
       "16    FL         Rick Scott      R     SenRickScott           131546062   \n",
       "78    SC      Lindsey Graham     R  LindseyGrahamSC           432895323   \n",
       "58    NJ         Cory Booker     D       CoryBooker            15808765   \n",
       "9     CA       Kamala Harris     D  SenKamalaHarris  803694179079458816   \n",
       "\n",
       "    Tweets  \n",
       "84    1632  \n",
       "33    1509  \n",
       "12    1334  \n",
       "32    1302  \n",
       "62    1078  \n",
       "17     440  \n",
       "16     411  \n",
       "78     372  \n",
       "58     363  \n",
       "9      336  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_counts_df.sort_values(by='Tweets', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the State Locations for Plotting Markers (1 of 3)\n",
    "* Get each **state’s latitude and longitude** coordinates for **plotting on a map**\n",
    "* **`state_codes.py`** contains a dictionary that maps **two-letter state codes** to **full state names**\n",
    "    * Used with **`geopy`** to look up the location of each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import OpenMapQuest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_codes import state_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the **`geocoder` object** to **translate location names** into **`Location` objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = OpenMapQuest(api_key=keys.mapquest_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-\n",
    "color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the State Locations for Plotting Markers (2 of 3)\n",
    "* Get and sort the unique state names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tweet_counts_df.State.unique()  # get unique state names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.sort() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the State Locations for Plotting Markers (3 of 3)\n",
    "* Look up **each state’s location**\n",
    "* Call `geocode` with state name followed by `', USA'` \n",
    "    * Ensures that we get United States locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyoming, United States of America\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for state in states:\n",
    "    processed = False\n",
    "    delay = .1 \n",
    "    while not processed:\n",
    "        try: \n",
    "            locations.append(geo.geocode(state_codes[state] + ', USA'))\n",
    "            clear_output()  # clear cell's current output before showing next one\n",
    "            print(locations[-1])  \n",
    "            processed = True\n",
    "        except:  # timed out, so wait before trying again\n",
    "            print('OpenMapQuest service timed out. Waiting.')\n",
    "            time.sleep(delay)\n",
    "            delay += .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the Tweet Counts by State \n",
    "* **Tweet total** for a states' two senators is used to **color the map**\n",
    "    * **Darker colors** represent **higher tweet counts**\n",
    "* **`DataFrame` method `groupby`** to group the senators by state \n",
    "    * **`as_index=False`**&mdash;state codes should be a column in returned **`GroupBy`** object, rather than indices for the object's rows\n",
    "* **`GroupBy`** object's **`sum`** method totals the numeric data by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_counts_by_state = tweet_counts_df.groupby(\n",
    "    'State', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Tweets\n",
       "0    AK      28\n",
       "1    AL      15\n",
       "2    AR      13\n",
       "3    AZ      31\n",
       "4    CA     397"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_counts_by_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap = folium.Map(location=[39.8283, -98.5795], \n",
    "                   zoom_start=4, detect_retina=True,\n",
    "                   tiles='Stamen Toner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Choropleth to Color the Map\n",
    "* A **choropleth** shades areas in a map using magnitudes of numerical values to determine color\n",
    "* For a **detailed description of the arguments** below, see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) (big data lesson coming soon) or [**Python for Programmers, Section 16.4.2** (under the heading \"Creating a Choropleth to Color the Map\"](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch16.xhtml#ch16lev2sec15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth = folium.Choropleth(\n",
    "    geo_data='us-states.json',\n",
    "    name='choropleth',\n",
    "    data=tweets_counts_by_state,\n",
    "    columns=['State', 'Tweets'],\n",
    "    key_on='feature.id',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Tweets by State'\n",
    ").add_to(usmap)\n",
    "\n",
    "layer = folium.LayerControl().add_to(usmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Map Markers for Each State (1 of 1)\n",
    "* Sort senators in **descending order** by **tweet count**\n",
    "* **`groupby`** maintains **original row order** in each group\n",
    "* **`index`** &mdash; used to look up each state’s location in **`locations` list**\n",
    "* **name** &mdash; two-letter **state code**\n",
    "* **`group`** &mdash; collection of a **state's two senators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = tweet_counts_df.sort_values(by='Tweets', ascending=False)\n",
    "\n",
    "for index, (name, group) in enumerate(sorted_df.groupby('State')):\n",
    "    strings = [state_codes[name]]  # used to assemble popup text\n",
    "\n",
    "    for s in group.itertuples():\n",
    "        strings.append(f'{s.Name} ({s.Party}); Tweets: {s.Tweets}')\n",
    "        \n",
    "    text = '<br>'.join(strings)  \n",
    "    popup = folium.Popup(text, max_width=200)\n",
    "    marker = folium.Marker(\n",
    "        (locations[index].latitude, locations[index].longitude), \n",
    "        popup=popup)\n",
    "    marker.add_to(usmap) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Displaying the Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "usmap.save('SenatorsTweets.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"./SenatorsTweets.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1222132e8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src=\"./SenatorsTweets.html\", width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.5 Hadoop\n",
    "* **Apache Hadoop** and **Apache Spark** deal with **big-data storage and processing challenges** via \n",
    "    * huge **clusters** of computers \n",
    "    * **distributed data storage** \n",
    "    * **massively parallel processing**\n",
    "    * Hadoop **MapReduce** programming\n",
    "    * Spark **in-memory processing** techniques\n",
    "* **Hadoop** also serves as the **foundation** for many recent advancements in **big-data processing** and an entire **ecosystem of software tools**\n",
    "\n",
    "<!--\n",
    "\n",
    "* For a list of **Hadoop ecosystem components**, see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411/) (big data lesson coming soon) or [**Python for Programmers, Section 16.5.1**](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch16.xhtml#ch16lev2sec16)\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.1 Hadoop Overview (1 of 2)\n",
    "* Early on, Google knew they needed to **return search results quickly**\n",
    "* The only practical way &mdash; **store and index the entire web** \n",
    "* Late 90s computers couldn’t store and analyze such a **large volume of data economically and fast enough**\n",
    "* Google developed a **clustering** system with vast numbers of computers (**nodes**)\n",
    "    * **greater chance of hardware failures** so they built in high levels of **redundancy** \n",
    "    * Data distributed across all these **commodity computers**\n",
    "* For a **search request**, all computers searched their portion of the web **in parallel**\n",
    "    * Then results were **gathered and returned**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.1 Hadoop Overview (2 of 2)\n",
    "* Google developed **clustering hardware and software** and **distributed storage**\n",
    "* **Published its designs**\n",
    "* Programmers at Yahoo!, working from Google’s [**“Google File System” paper**](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf), built their own system\n",
    "* They **open-sourced** their work and **Apache.org** implemented **Hadoop**\n",
    "* Named for an **elephant** stuffed animal that belonged to a child of one of Hadoop’s creators\n",
    "    * Inspiration for our textbook cover\n",
    "* Two **additional Google papers** contributed to evolution of Hadoop\n",
    "\t* [**“MapReduce: Simplified Data Processing on Large Clusters”**](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) \n",
    "\t* [**“Bigtable: A Distributed Storage System for Structured Data”**](http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) \n",
    "\t\t* Basis for Apache HBase (a NoSQL key–value and column-based database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS, `MapReduce` and YARN (1 of 2)\n",
    "* Hadoop’s key components are: \n",
    "\t* **HDFS** (Hadoop Distributed File System) for **storing massive amounts of data** throughout a **cluster** \n",
    "\t* **MapReduce** for implementing the **tasks that process the data**\n",
    "        * Like **filter/map/reduce** in functional-style programming, but **massively parallel**\n",
    "* **MapReduce** performs two steps—**mapping** and **reduction**\n",
    "    * **Mapping** (and **filtering**) &mdash; processes original data across **entire cluster** and **maps** it into tuples of **key–value pairs**\n",
    "    * **Reduction** &mdash; **combines** those tuples to **produce the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS, `MapReduce` and YARN (2 of 2)\n",
    "* Hadoop divides data into **batches** and **distributes** across **cluster's nodes**\n",
    "* Also **distributes MapReduce code** to **execute in parallel on every node**\n",
    "    * Each **node** processes the **batch of data** stored **on that node**\n",
    "* **Reduction combines results** from **all nodes** to produce **final result**\n",
    "* **YARN** (“yet another resource negotiator”) **manages all resources** in the **cluster** and **schedules tasks** for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Ecosystem \n",
    "* **Hadoop began** with **HDFS** and **MapReduce**, followed closely by **YARN**\n",
    "* Now it's a **large ecosystem**, including **Hadoop 3** (3.2 released in January), **Spark** and many other Apache projects[\\[1\\]](https://hortonworks.com/ecosystems/),[\\[2\\]](https://readwrite.com/2018/06/26/complete-guide-of-hadoop-ecosystem-components/),[\\[3\\]](https://www.janbasktraining.com/blog/introduction-architecture-components-hadoop-ecosystem/)\n",
    "* [See our table of Hadoop Ecosystem Components](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.2 Summarizing Word Lengths in _Romeo and Juliet_ via MapReduce\n",
    "* To execute this example\n",
    "    * We created a **cloud-based, multi-node cluster of computers** using **Microsoft Azure HDInsight**, which provides **Hadoop as a service**\n",
    "    * Ran our **Hadoop MapReduce** code on that **cluster**\n",
    "* **MapReduce task** \n",
    "    * Determines the **length of each word in `RomeoAndJuliet.txt`** (from the NLP presentation)\n",
    "    * Summarizes **number of words of each length** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.3 Creating an Apache Hadoop Cluster in Microsoft Azure HDInsight (1 of 2)\n",
    "* **Multi-node cloud-based clusters** typically are **paid** services\n",
    "* We used [**Microsoft Azure’s HDInsight service**](https://azure.microsoft.com/en-us/free) (for Hadoop and Spark) and **new account credit** to create **cloud-based clusters of computers** in which to test our examples\n",
    "    * The **new account credit** was more than enough to test our Hadoop and Spark examples\n",
    "* HDInsight service **requires a credit card** for **identity verification**\n",
    "    * When your **new account credit runs out** or **30 days pass**, cannot use paid services unless you authorize Microsoft to **charge your card**\n",
    "        * **Prevents accidental large bills** for people who are just experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.3 Creating an Apache Hadoop Cluster in Microsoft Azure HDInsight (2 of 2)\n",
    "* Details on configuring a **low-cost cluster** to try **Hadoop** are discussed in my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) (big data lesson coming soon) and in [**Python for Programmers, Section 16.5.3**](https://learning.oreilly.com/library/view/Python+for+Programmers,+First+Edition/9780135231364/ch16.xhtml#ch16lev2sec18) \n",
    "    * [See Microsoft’s **recommended cluster configurations**](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning#default-node-configuration-and-virtual-machine-sizes-for-clusters)\n",
    "* **Caution: Once you allocate a cluster, it incurs costs whether you’re using it or not. So, when you complete this case study, be sure to delete your cluster(s) and other resources, so you don’t incur additional charges.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.4 Hadoop Streaming\n",
    "* Hadoop is **Java-based**\n",
    "* Languages like **Python** that are **not natively supported** must use **Hadoop streaming** \n",
    "* **Python MapReduce scripts** communicate with Hadoop via **redirected standard I/O streams**\n",
    "\t* Hadoop redirects **input** to **mapper** script, which reads input from **standard input stream**\n",
    "\t* **Mapper** writes results to **standard output stream**\n",
    "\t* Hadoop redirects **mapper’s output** as **input** to **reducer** script, which reads from the **standard input stream**\n",
    "\t* **Reducer** writes results to **standard output stream** \n",
    "\t* Hadoop writes **reducer’s output** to **HDFS**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.5 Implementing the Mapper (1 of 2)\n",
    "* **Mapper** takes **lines of text** as input and **maps** them to **key–value pairs**, each containing a **word length** and **`1`**\n",
    "* **Reducer** will total these **key–value pairs** by key\n",
    "* **Hadoop streaming** expects **mapper’s output** and **reducer’s input/output** to have the form \n",
    "> **_key_`\\t`_value_**\n",
    "* In **`length_mapper.py`**, `#!` tells Hadoop to use Python 3\n",
    "    * Must be first line in the file\n",
    "    * **HDInsight** currently includes **Python 2.7.12** and **Python 3.5.2** \n",
    "    * **Cannot use f-strings** which are **Python 3.6+**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.5 Implementing the Mapper (2 of 2)\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# length_mapper.py\n",
    "\"\"\"Maps lines of text to key-value pairs of word lengths and 1.\"\"\"\n",
    "import sys\n",
    "\n",
    "def tokenize_input():  # generator function\n",
    "    \"\"\"Split each line of standard input into a list of strings.\"\"\"\n",
    "    for line in sys.stdin:\n",
    "        yield line.split()  \n",
    "\n",
    "# read each line in the the standard input and for every word \n",
    "# produce a key-value pair containing the word, a tab and 1\n",
    "for line in tokenize_input():\n",
    "    for word in line:\n",
    "        print(str(len(word)) + '\\t1')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.6 Implementing the Reducer \n",
    "* In **`length_reducer.py`**, function **`tokenize_input`** is a **generator function** that reads and splits the **key–value pairs** produced by the mapper\n",
    "* **MapReduce streaming** supplies the **standard input**\n",
    "* **`groupby` function** (**`itertools` module**) groups inputs by their **keys** (the **word lengths**) \n",
    "* **Total** all the **counts** for a given **key**\n",
    "* Output a new **key–value pair** consisting of the **word length** and its **total**\n",
    "* **MapReduce** takes the **final word-count outputs** and **writes** them to a file in **HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# length_reducer.py\n",
    "\"\"\"Counts the number of words with each length.\"\"\"\n",
    "import sys\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def tokenize_input():\n",
    "    \"\"\"Split each line of standard input into a key and a value.\"\"\"\n",
    "    for line in sys.stdin:\n",
    "        yield line.strip().split('\\t')\n",
    "\n",
    "# produce key-value pairs of word lengths and counts separated by tabs\n",
    "for word_length, group in groupby(tokenize_input(), itemgetter(0)):\n",
    "    try:\n",
    "        total = sum(int(count) for word_length, count in group)\n",
    "        print(word_length + '\\t' + str(total))\n",
    "    except ValueError:\n",
    "        pass  # ignore word if its count was not an integer\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.7 Preparing to Run the MapReduce Example\n",
    "* Must upload scripts and `RomeoAndJuliet.txt` into **HDInsight cluster's file system**\n",
    "* For detailed instructions on doing this, see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) or [**Python for Programmers, Section 16.5.7**](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch16.xhtml#ch16lev2sec22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.8 Running the MapReduce Job (1 of 4)\n",
    "* Once files are in the cluster, **run MapReduce job** for **`RomeoAndJuliet.txt`** on your **cluster** by executing the following command in the cluster\n",
    "    * You can copy/paste the command from `yarn.txt` located with this example\n",
    "    * We reformatted the command here for readability: \n",
    "\n",
    "```\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \n",
    "   -D mapred.output.key.comparator.class=\n",
    "      org.apache.hadoop.mapred.lib.KeyFieldBasedComparator    \n",
    "   -D mapred.text.key.comparator.options=-n   \n",
    "   -files length_mapper.py,length_reducer.py    \n",
    "   -mapper length_mapper.py \n",
    "   -reducer length_reducer.py    \n",
    "   -input /example/data/RomeoAndJuliet.txt    \n",
    "   -output /example/wordlengthsoutput    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.8 Running the MapReduce Job (2 of 4)\n",
    "* The **`yarn` command** invokes **Hadoop’s YARN** (“yet another resource negotiator”) tool to manage and coordinate access to the Hadoop resources the **MapReduce task** uses\n",
    "* **`hadoop-streaming.jar`** contains the Java-based **Hadoop streaming utility** that allows you to use Python to implement the **mapper** and **reducer**\n",
    "* The two **`-D` options** set Hadoop properties that enable it to \n",
    "    * **sort the final key–value pairs by key** (**`KeyFieldBasedComparator`**) \n",
    "    * in **descending order (`-`) numerically (`n`)** rather than alphabetically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.8 Running the MapReduce Job (3 of 4)\n",
    "* Other command-line arguments:\n",
    "\t* **`-files`**—Comma-separated list of scripts that Hadoop copies to every node in the cluster so they can **execute locally on each node**\n",
    "\t* **`-mapper`**—mapper’s script file\n",
    "\t* **`-reducer`**—reducer’s script file\n",
    "\t* **`-input`**—**File** or **directory of files** to supply as **mapper input**\n",
    "\t* **`-output`**—**HDFS directory** where final results will be stored\n",
    "        * **Error** if this folder **already exists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.5.8 Running the MapReduce Job (4 of 4)\n",
    "* Sample **output** shows some **Hadoop feedback** produced as the **MapReduce job executes**\n",
    "    * Used **`...`** to save space \n",
    "* Several lines of interest:\n",
    "\t* The total number of **“input paths to process”**—the **`1` source of input** in this example is **`RomeoAndJuliet.txt`**\n",
    "\t* The **“number of splits”**—`2` in this example, based on the **number of worker nodes** in our **HDInsight cluster**\n",
    "\t* The **percentage completion** dates and times&mdash;**big data jobs** could take minutes, hours, days, ...\n",
    "\t* **`File System Counters`** showing numbers of **bytes read and written**\n",
    "\t* **`Job Counters`** showing the **numbers of mapping and reduction tasks used** \n",
    "\t* **`Map-Reduce Framework`** showing stats about the **steps performed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "<pre>\n",
    "packageJobJar: [] [/usr/hdp/2.6.5.3004-13/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.3004-13.jar] /tmp/streamjob2764990629848702405.jar tmpDir=null\n",
    "...\n",
    "18/12/05 16:46:25 INFO mapred.FileInputFormat: <strong>Total input paths to process : 1</strong>\n",
    "18/12/05 16:46:26 INFO mapreduce.JobSubmitter: <strong>number of splits:2</strong>\n",
    "...\n",
    "18/12/05 16:46:26 INFO mapreduce.Job: The url to track the job: http://hn0-paulte.y3nghy5db2kehav5m0opqrjxcb.cx.internal.cloudapp.net:8088/proxy/application_1543953844228_0025/\n",
    "...\n",
    "18/12/05 16:46:35 INFO mapreduce.Job:  <strong>map 0% reduce 0%</strong>\n",
    "18/12/05 16:46:43 INFO mapreduce.Job:  <strong>map 50% reduce 0%</strong>\n",
    "18/12/05 16:46:44 INFO mapreduce.Job:  <strong>map 100% reduce 0%</strong>\n",
    "18/12/05 16:46:48 INFO mapreduce.Job:  <strong>map 100% reduce 100%</strong>\n",
    "18/12/05 16:46:50 INFO mapreduce.Job: Job job_1543953844228_0025 <strong>completed successfully</strong>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "18/12/05 16:46:50 INFO mapreduce.Job: Counters: 49\n",
    "\t<strong>File System Counters</strong>\n",
    "\t\tFILE: Number of bytes read=156411\n",
    "\t\tFILE: Number of bytes written=813764\n",
    "...\n",
    "\t<strong>Job Counters</strong> \n",
    "\t\tLaunched map tasks=2\n",
    "\t\tLaunched reduce tasks=1\n",
    "...\n",
    "\t<strong>Map-Reduce Framework</strong>\n",
    "\t\tMap input records=5260\n",
    "\t\tMap output records=25956\n",
    "\t\tMap output bytes=104493\n",
    "\t\tMap output materialized bytes=156417\n",
    "\t\tInput split bytes=346\n",
    "\t\tCombine input records=0\n",
    "\t\tCombine output records=0\n",
    "\t\tReduce input groups=19\n",
    "\t\tReduce shuffle bytes=156417\n",
    "\t\tReduce input records=25956\n",
    "\t\tReduce output records=19\n",
    "\t\tSpilled Records=51912\n",
    "\t\tShuffled Maps =2\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=2\n",
    "\t\tGC time elapsed (ms)=193\n",
    "\t\tCPU time spent (ms)=4440\n",
    "\t\tPhysical memory (bytes) snapshot=1942798336\n",
    "\t\tVirtual memory (bytes) snapshot=8463282176\n",
    "\t\tTotal committed heap usage (bytes)=3177185280\n",
    "...\n",
    "18/12/05 16:46:50 INFO streaming.StreamJob: Output directory: /example/wordlengthsoutput\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Word Counts\n",
    "* **Hadoop MapReduce** saves its output into **HDFS**\n",
    "* To **view final word counts** you must look at the **file in cluster's HDFS**\n",
    "> ```\n",
    "hdfs dfs -text /example/wordlengthsoutput/part-00000\n",
    "```\n",
    "\n",
    "```\n",
    "18/12/05 16:47:19 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
    "18/12/05 16:47:19 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev b5efb3e531bc1558201462b8ab15bb412ffa6b89]\n",
    "1\t4699\n",
    "2\t3869\n",
    "3\t5651\n",
    "4\t3668\n",
    "5\t2719\n",
    "6\t1624\n",
    "7\t1140\n",
    "8\t1062\n",
    "9\t855\n",
    "10\t317\n",
    "11\t189\n",
    "12\t95\n",
    "13\t35\n",
    "14\t13\n",
    "15\t9\n",
    "16\t6\n",
    "17\t3\n",
    "18\t1\n",
    "23\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: Deleting Your Cluster So You Do Not Incur Charges\n",
    "* **Be sure to delete your cluster(s) and associated resources (like storage) so you don’t incur additional charges** \n",
    "    * For details on deleting the cluster, see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) (big data lesson coming soon) and in [**Python for Programmers, Section 16.5.8**](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch16.xhtml#ch16lev2sec23)\n",
    "* [More information](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-portal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Note: Switch to **jupyter/pyspark-notebook Docker stack** for Spark presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:#000; background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2019 by Pearson Education, Inc. All Rights Reserved. The content in this notebook is based on the book [**Python for Programmers**](https://amzn.to/2VvdnxE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
